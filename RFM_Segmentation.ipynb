{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5596,
          "sourceType": "datasetVersion",
          "datasetId": 3466
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "RFM-Segmentation",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/technologyhamed/Apache-PySpark/blob/main/RFM_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'onlineretail:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3466%2F5596%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240820%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240820T171538Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D67dec48420433cb13eb0679c06763815511f86d12a05670491b8620ad8d288819279cdab4f058c79a32991b14fc5bb267be3e0ec6c0f6575b7e1a3b23bca93a0dc0e2fededa2f61fd025cd583b6c5b7e4ee5ffac6a92f4e35f84a029219a90ad598685ec495f5c3025894b0441ff009443cb1449b406d8eca68717b03b6c847fdf52692dbfcd151a3fae2493273c3e41ac84e7663a2f0cd5a1c91e323119611eb5633fea055af6938c74d38fa47f0c66397ea7a0565c531083069f4604fe8b4b05752185ba897f4f5ac3bf8f37895ff99fa15f418bb5703f6536c65038afa94676946365eee80ca6a8a08084aac5da9a509e41f7974621426025e81adef6b11f'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
      ],
      "metadata": {
        "id": "71ZgOa4oy_9d",
        "outputId": "1dea34a0-1892-4355-ed0d-d669c00b1a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading onlineretail, 7548702 bytes compressed\n",
            "[==================================================] 7548702 bytes downloaded\n",
            "Downloaded and uncompressed: onlineretail\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install jdatetime"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T16:00:24.974201Z",
          "iopub.execute_input": "2024-08-20T16:00:24.974593Z",
          "iopub.status.idle": "2024-08-20T16:01:29.779297Z",
          "shell.execute_reply.started": "2024-08-20T16:00:24.974552Z",
          "shell.execute_reply": "2024-08-20T16:01:29.777915Z"
        },
        "trusted": true,
        "id": "sF6DIMiey_9j",
        "outputId": "f04d1457-1ecb-4eb4-f877-942f300de4d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: jdatetime in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: jalali-core>=1.0 in /usr/local/lib/python3.10/dist-packages (from jdatetime) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****RFM Analysis****\n",
        "\n",
        "The above figure source: Blast Analytics Marketing\n",
        "\n",
        "RFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in retail and professional services industries. More details can be found at Wikipedia RFM_wikipedia.\n",
        "\n",
        "RFM stands for the three dimensions:\n",
        "\n",
        "Recency – How recently did the customer purchase? i.e. Duration since last purchase\n",
        "\n",
        "Frequency – How often do they purchase? i.e. Total number of purchases\n",
        "\n",
        "Monetary Value – How much do they spend? i.e. Total money this customer spent"
      ],
      "metadata": {
        "id": "Tl9Y-uE4y_9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark RFM example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "trusted": true,
        "id": "neqCilvzy_9p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = spark.read.format('com.databricks.spark.csv').\\\n",
        "                       options(header='true', \\\n",
        "                       inferschema='true').\\\n",
        "            load(\"/kaggle/input/onlineretail/OnlineRetail.csv\",header=True);"
      ],
      "metadata": {
        "trusted": true,
        "id": "XAIT-Beyy_9p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_raw.show(5)\n",
        "df_raw.printSchema()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T10:10:31.404439Z",
          "iopub.execute_input": "2024-08-20T10:10:31.405692Z",
          "iopub.status.idle": "2024-08-20T10:10:31.413125Z",
          "shell.execute_reply.started": "2024-08-20T10:10:31.405652Z",
          "shell.execute_reply": "2024-08-20T10:10:31.411664Z"
        },
        "trusted": true,
        "id": "6-U1lghky_9q",
        "outputId": "0f3a7ba8-934a-48c1-c00e-eeb768bdff8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data clean and data manipulation**\n",
        "\n",
        "check and remove the null values"
      ],
      "metadata": {
        "id": "rk2lmOjKy_9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "def my_count(df_in):\n",
        "    df_in.agg( *[ count(c).alias(c) for c in df_in.columns ] ).show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T09:49:41.562693Z",
          "iopub.execute_input": "2024-08-20T09:49:41.563169Z",
          "iopub.status.idle": "2024-08-20T09:49:41.570225Z",
          "shell.execute_reply.started": "2024-08-20T09:49:41.563136Z",
          "shell.execute_reply": "2024-08-20T09:49:41.568697Z"
        },
        "trusted": true,
        "id": "hLq5YXQZy_9s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_count(df_raw)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T10:10:40.013884Z",
          "iopub.execute_input": "2024-08-20T10:10:40.01432Z",
          "iopub.status.idle": "2024-08-20T10:10:41.650485Z",
          "shell.execute_reply.started": "2024-08-20T10:10:40.014286Z",
          "shell.execute_reply": "2024-08-20T10:10:41.649305Z"
        },
        "trusted": true,
        "id": "XzjyoiGiy_9s",
        "outputId": "e17fbbd7-fd53-493c-9aab-58b565713933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "|   541909|   541909|     540455|  541909|     541909|   541909|    406829| 541909|\n",
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since the count results are not the same, we have some null value in the CustomerID column. We can drop these records from the dataset.**"
      ],
      "metadata": {
        "id": "ErcSw-WTy_9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_raw.dropna(how='any')\n",
        "my_count(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T10:50:05.19577Z",
          "iopub.execute_input": "2024-08-20T10:50:05.196226Z",
          "iopub.status.idle": "2024-08-20T10:50:06.880863Z",
          "shell.execute_reply.started": "2024-08-20T10:50:05.196193Z",
          "shell.execute_reply": "2024-08-20T10:50:06.87952Z"
        },
        "trusted": true,
        "id": "5wEwun_My_9t",
        "outputId": "0c9a99d9-06af-41c5-90ff-24f3b60dd0bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "|   406829|   406829|     406829|  406829|     406829|   406829|    406829| 406829|\n",
            "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "import jdatetime\n",
        "import datetime  # Import the datetime module\n",
        "\n",
        "# Define a UDF to convert a datetime string to a Persian datetime\n",
        "@udf(StringType())\n",
        "def convert_to_persian_datetime(datetime_str):\n",
        "    try:\n",
        "        # Parse the datetime string to a Python datetime object\n",
        "        dt = datetime.datetime.strptime(datetime_str, \"%Y-%m-%d\")\n",
        "\n",
        "        # Convert the Python datetime object to a Persian datetime object\n",
        "        persian_dt = jdatetime.datetime(dt.year, dt.month, dt.day)\n",
        "\n",
        "        # Format the Persian datetime object as a string\n",
        "        persian_datetime_str = persian_dt.strftime(\"%Y/%m/%d\")\n",
        "\n",
        "        return persian_datetime_str\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# Create a DataFrame with a datetime string column\n",
        "df = spark.createDataFrame([(\"2024-08-20\",)], [\"datetime_str\"])\n",
        "\n",
        "# Apply the UDF to convert the datetime string to a Persian datetime\n",
        "df_with_persian_datetime = df.withColumn(\"persian_datetime\", convert_to_persian_datetime(\"datetime_str\"))\n",
        "\n",
        "# Show the result\n",
        "df_with_persian_datetime.show()"
      ],
      "metadata": {
        "id": "rAOh7_PEMPMr",
        "outputId": "739478f3-4af1-4c32-eb9e-1ee9784e3fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------------+\n",
            "|datetime_str|persian_datetime|\n",
            "+------------+----------------+\n",
            "|  2024-08-20|      2024/08/20|\n",
            "+------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import unix_timestamp, to_utc_timestamp, col\n",
        "\n",
        "timeFmt = \"MM/dd/yy HH:mm\"\n",
        "\n",
        "df = df.withColumn('NewInvoiceDate'\n",
        "                 , to_utc_timestamp(unix_timestamp(col('InvoiceDate'),timeFmt).cast('timestamp'),'UTC'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T10:50:16.738699Z",
          "iopub.execute_input": "2024-08-20T10:50:16.739122Z",
          "iopub.status.idle": "2024-08-20T10:50:16.762812Z",
          "shell.execute_reply.started": "2024-08-20T10:50:16.73909Z",
          "shell.execute_reply": "2024-08-20T10:50:16.761204Z"
        },
        "trusted": true,
        "id": "zpXMWbYPy_9t"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The spark is pretty sensitive to the date format!**"
      ],
      "metadata": {
        "id": "_Q5S3JPby_9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **calculate total price**"
      ],
      "metadata": {
        "id": "GWZd7nx3y_9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import round\n",
        "\n",
        "df = df.withColumn('TotalPrice', round( df.Quantity * df.UnitPrice, 2 ) )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T10:50:30.531281Z",
          "iopub.execute_input": "2024-08-20T10:50:30.532443Z",
          "iopub.status.idle": "2024-08-20T10:50:30.554696Z",
          "shell.execute_reply.started": "2024-08-20T10:50:30.5324Z",
          "shell.execute_reply": "2024-08-20T10:50:30.553412Z"
        },
        "trusted": true,
        "id": "pFtMgUjMy_9u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, min, max, sum, datediff, to_date, lit, unix_timestamp, to_utc_timestamp\n",
        "\n",
        "# Set legacy time parser policy for compatibility\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "date_max = df.select(max('NewInvoiceDate')).toPandas()\n",
        "current = to_utc_timestamp(unix_timestamp(lit(str(date_max.iloc[0][0])), \\\n",
        "                              'yy-MM-dd HH:mm').cast('timestamp'), 'UTC' )\n",
        "\n",
        "# Calculatre Duration\n",
        "df = df.withColumn('Duration', datediff(lit(current), 'NewInvoiceDate'))\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "W3uMLNfSGPEX",
        "outputId": "f1c0163c-6bff-4368-c9a5-e17b155d6503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-51dfabd6cb79>:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  current = to_utc_timestamp(unix_timestamp(lit(str(date_max.iloc[0][0])), \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+----------+--------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|     NewInvoiceDate|TotalPrice|Duration|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+----------+--------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01 08:26:00|      15.3|     373|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01 08:26:00|     20.34|     373|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01 08:26:00|      22.0|     373|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01 08:26:00|     20.34|     373|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01 08:26:00|     20.34|     373|\n",
            "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recency = df.groupBy('CustomerID').agg(min('Duration').alias('Recency'))\n",
        "frequency = df.groupBy('CustomerID', 'InvoiceNo').count()\\\n",
        "                        .groupBy('CustomerID')\\\n",
        "                        .agg(count(\"*\").alias(\"Frequency\"))\n",
        "monetary = df.groupBy('CustomerID').agg(round(sum('TotalPrice'), 2).alias('Monetary'))\n",
        "rfm = recency.join(frequency,'CustomerID', how = 'inner')\\\n",
        "             .join(monetary,'CustomerID', how = 'inner')"
      ],
      "metadata": {
        "trusted": true,
        "id": "7KuAyADjy_9u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfm.show(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "k1sI06n1y_9u",
        "outputId": "4d875a55-09ac-4638-ccb1-2db2a0b625be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+---------+--------+\n",
            "|CustomerID|Recency|Frequency|Monetary|\n",
            "+----------+-------+---------+--------+\n",
            "|     17389|      0|       43|31300.08|\n",
            "|     13623|     30|        7|  672.44|\n",
            "|     14450|    180|        3|  483.25|\n",
            "|     15727|     16|        7| 5178.96|\n",
            "|     13285|     23|        4| 2709.12|\n",
            "+----------+-------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.2.2. RFM Segmentation**\n",
        "\n",
        "> Determine cutting points\n",
        "In this section, you can use the techniques (statistical results and visualizations) in Data Exploration section to help you determine the cutting points for each attribute. In my opinion, the cutting points are mainly depend on the business sense. You’s better talk to your makrting people and get feedback and suggestion from them. I will use the quantile as the cutting points in this demo."
      ],
      "metadata": {
        "id": "luA6KMxLy_9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def describe_pd(spark_df, cols, decimals):\n",
        "    \"\"\"\n",
        "    Calculates descriptive statistics for specified columns in a PySpark DataFrame\n",
        "    and displays them in a pandas-like format.\n",
        "\n",
        "    Args:\n",
        "        spark_df: The PySpark DataFrame.\n",
        "        cols: A list of column names to calculate statistics for.\n",
        "        decimals: Number of decimal places to round to.\n",
        "    \"\"\"\n",
        "    summary = spark_df.select(cols).summary()\n",
        "    pandas_df = summary.toPandas().set_index(\"summary\")\n",
        "    pandas_df = pandas_df.round(decimals)\n",
        "    display(pandas_df)\n",
        "\n",
        "cols = ['Recency','Frequency','Monetary']\n",
        "describe_pd(rfm, cols, 1)"
      ],
      "metadata": {
        "id": "bv_kV5G5y_9v",
        "outputId": "d6773bac-21ca-4008-91ac-2c431f88804b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                    Recency         Frequency            Monetary\n",
              "summary                                                          \n",
              "count                  4372              4372                4372\n",
              "mean      91.58119853613907  5.07548032936871  1898.4597003659621\n",
              "stddev   100.77213931384827  9.33875416357473    8219.34514113974\n",
              "min                       0                 1            -4287.63\n",
              "25%                      16                 1               293.1\n",
              "50%                      50                 3              647.74\n",
              "75%                     143                 5             1611.59\n",
              "max                     373               248           279489.02"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a511c583-7e91-46d1-95bd-05053572d517\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Recency</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Monetary</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>summary</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4372</td>\n",
              "      <td>4372</td>\n",
              "      <td>4372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>91.58119853613907</td>\n",
              "      <td>5.07548032936871</td>\n",
              "      <td>1898.4597003659621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stddev</th>\n",
              "      <td>100.77213931384827</td>\n",
              "      <td>9.33875416357473</td>\n",
              "      <td>8219.34514113974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-4287.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>293.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>50</td>\n",
              "      <td>3</td>\n",
              "      <td>647.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>143</td>\n",
              "      <td>5</td>\n",
              "      <td>1611.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>373</td>\n",
              "      <td>248</td>\n",
              "      <td>279489.02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a511c583-7e91-46d1-95bd-05053572d517')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a511c583-7e91-46d1-95bd-05053572d517 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a511c583-7e91-46d1-95bd-05053572d517');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c0cbea75-b6c7-45c4-a101-f5fbe75ff7c0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0cbea75-b6c7-45c4-a101-f5fbe75ff7c0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c0cbea75-b6c7-45c4-a101-f5fbe75ff7c0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"describe_pd(rfm, cols, 1)\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"mean\",\n          \"50%\",\n          \"count\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recency\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"91.58119853613907\",\n          \"50\",\n          \"4372\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Frequency\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"4372\",\n          \"5.07548032936871\",\n          \"5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Monetary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"1898.4597003659621\",\n          \"647.74\",\n          \"4372\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The user defined function by using the cutting points:"
      ],
      "metadata": {
        "id": "JSyaNW1Ky_9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RScore(x):\n",
        "    if  x <= 16:\n",
        "        return 1\n",
        "    elif x<= 50:\n",
        "        return 2\n",
        "    elif x<= 143:\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "\n",
        "def FScore(x):\n",
        "    if  x <= 1:\n",
        "        return 4\n",
        "    elif x <= 3:\n",
        "        return 3\n",
        "    elif x <= 5:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def MScore(x):\n",
        "    if  x <= 293:\n",
        "        return 4\n",
        "    elif x <= 648:\n",
        "        return 3\n",
        "    elif x <= 1611:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ne985Xo_y_9v"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "\n",
        "R_udf = udf(lambda x: RScore(x), StringType())\n",
        "F_udf = udf(lambda x: FScore(x), StringType())\n",
        "M_udf = udf(lambda x: MScore(x), StringType())"
      ],
      "metadata": {
        "id": "glICjt-Ey_9v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****RFM Segmentation****"
      ],
      "metadata": {
        "id": "u8TRCNS8y_9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfm_seg = rfm.withColumn(\"r_seg\", R_udf(\"Recency\"))\n",
        "rfm_seg = rfm_seg.withColumn(\"f_seg\", F_udf(\"Frequency\"))\n",
        "rfm_seg = rfm_seg.withColumn(\"m_seg\", M_udf(\"Monetary\"))\n",
        "rfm_seg.show(5)"
      ],
      "metadata": {
        "id": "K23oEyAKy_9w",
        "outputId": "039491b9-b555-4078-bf65-d502669ec590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+---------+--------+-----+-----+-----+\n",
            "|CustomerID|Recency|Frequency|Monetary|r_seg|f_seg|m_seg|\n",
            "+----------+-------+---------+--------+-----+-----+-----+\n",
            "|     17389|      0|       43|31300.08|    1|    1|    1|\n",
            "|     13623|     30|        7|  672.44|    2|    1|    2|\n",
            "|     14450|    180|        3|  483.25|    4|    3|    3|\n",
            "|     15727|     16|        7| 5178.96|    1|    1|    1|\n",
            "|     13285|     23|        4| 2709.12|    2|    2|    1|\n",
            "+----------+-------+---------+--------+-----+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, concat, col\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "rfm_seg = rfm_seg.withColumn('RFMScore',\n",
        "                             concat(col('r_seg'),col('f_seg'), col('m_seg'))) # Use concat and col from pyspark.sql.functions\n",
        "rfm_seg.sort(col('RFMScore')).show(5)"
      ],
      "metadata": {
        "id": "NDVPaZkny_9w",
        "outputId": "a198ecc7-e497-4652-fadf-1a41af1167a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+---------+--------+-----+-----+-----+--------+\n",
            "|CustomerID|Recency|Frequency|Monetary|r_seg|f_seg|m_seg|RFMScore|\n",
            "+----------+-------+---------+--------+-----+-----+-----+--------+\n",
            "|     18161|     10|        6| 1612.79|    1|    1|    1|     111|\n",
            "|     12471|      2|       49|18740.92|    1|    1|    1|     111|\n",
            "|     17389|      0|       43|31300.08|    1|    1|    1|     111|\n",
            "|     15727|     16|        7| 5178.96|    1|    1|    1|     111|\n",
            "|     17754|      0|        6| 1739.92|    1|    1|    1|     111|\n",
            "+----------+-------+---------+--------+-----+-----+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.2.3. Statistical Summary**\n",
        "\n",
        "simple summary"
      ],
      "metadata": {
        "id": "ocXrcWLiy_9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F  # Import and alias pyspark.sql.functions as F\n",
        "\n",
        "rfm_seg.groupBy('RFMScore')\\\n",
        "    .agg({'Recency':'mean',\n",
        "          'Frequency': 'mean',\n",
        "          'Monetary': 'mean'} )\\\n",
        "    .sort(F.col('RFMScore')).show(5)  # Now you can use F.col"
      ],
      "metadata": {
        "id": "L90kAKzCy_9w",
        "outputId": "0920eed8-d486-4b94-fdd8-a7050bd84c27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------------+------------------+------------------+\n",
            "|RFMScore|     avg(Recency)|     avg(Monetary)|    avg(Frequency)|\n",
            "+--------+-----------------+------------------+------------------+\n",
            "|     111|6.035123966942149| 8828.888595041319|18.882231404958677|\n",
            "|     112|7.237113402061856|1223.3604123711339| 7.752577319587629|\n",
            "|     113|              8.0|          505.9775|               7.5|\n",
            "|     114|             11.0|            191.17|               8.0|\n",
            "|     121|6.472727272727273|2569.0619999999994| 4.636363636363637|\n",
            "+--------+-----------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complex summery**"
      ],
      "metadata": {
        "id": "bqzoOLWqy_9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "def quantile_agg(df_input, grp, num_cols):\n",
        "    # Implement your quantile aggregation logic here\n",
        "    # For example:\n",
        "    result = df_input.groupBy(grp).agg(\n",
        "        *[F.percentile_approx(col, [0.25, 0.5, 0.75]).alias(col + '_quantiles') for col in num_cols]\n",
        "    )\n",
        "    return result\n",
        "\n",
        "def deciles_agg(df_input, grp, num_cols):\n",
        "    # Implement your deciles aggregation logic here\n",
        "    # For example:\n",
        "    result = df_input.groupBy(grp).agg(\n",
        "        *[F.percentile_approx(col, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]).alias(col + '_deciles') for col in num_cols]\n",
        "    )\n",
        "    return result\n",
        "\n",
        "grp = 'RFMScore'\n",
        "num_cols = ['Recency','Frequency','Monetary']\n",
        "df_input = rfm_seg\n",
        "outputdir='/content/sample_data/outputdir/'\n",
        "\n",
        "quantile_grouped = quantile_agg(df_input,grp,num_cols)\n",
        "quantile_grouped.toPandas().to_csv(outputdir + 'quantile_grouped.csv')\n",
        "\n",
        "deciles_grouped = deciles_agg(df_input,grp,num_cols)\n",
        "deciles_grouped.toPandas().to_csv(outputdir + 'deciles_grouped.csv')"
      ],
      "metadata": {
        "id": "-1kILDcTPIMD"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}